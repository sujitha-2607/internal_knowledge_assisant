# Internal Knowledge Assistant

This project is an internal knowledge assistant built to answer questions based on a private collection of documents. It uses a Retrieval-Augmented Generation (RAG) pipeline to provide accurate, context-aware answers.

The application is composed of three main services: a FastAPI backend, a Streamlit frontend, and a local Large Language Model (LLM) powered by Ollama.

## ‚ú® Features

-   **RAG Pipeline**: Utilizes a Sentence Transformer model for document embedding and FAISS for efficient vector search.
-   **Document Ingestion**: Supports uploading `.txt` and `.pdf` files, which are automatically processed and indexed.
-   **Efficient Indexing**: Features incremental indexing, allowing new files to be added without rebuilding the entire knowledge base.
-   **Local LLM**: Uses a locally-run Ollama server with the Gemma 1B model to generate answers.
-   **FastAPI Backend**: Provides a robust API for managing document uploads and handling user queries.
-   **Streamlit Frontend**: A simple, user-friendly web interface for interacting with the assistant.
-   **Streaming Responses**: The UI displays answers in real-time as they are generated by the LLM.
-   **Source Attribution**: Provides source file names and chunk numbers for every answer, ensuring transparency and trustworthiness.

## üöÄ How to Run Locally

Follow these steps to set up and run the project on your local machine.

### Prerequisites

-   **Python 3.8+**: Ensure you have Python installed.
-   **Ollama**: Install Ollama from [ollama.com](https://ollama.com/).
-   **gemma3:1b Model**: Pull the Gemma 1B model using the command:
    ```bash
    ollama pull gemma3:1b
    ```

### Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/Tvamsiprakash/internal-knowledge-assistant.git]
    cd Internal-Knowledge-Assistant
    ```

2.  **Create a virtual environment and install dependencies:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows, use `.venv\Scripts\activate`
    pip install -r requirements.txt
    ```

### Usage

1.  **Start the FastAPI backend:**
    Ensure you are in the project's root directory and run:
    ```bash
    uvicorn app.main:app --reload --port 8000
    ```
    This will start the API server at `http://127.0.0.1:8000`.

2.  **Start the Streamlit frontend:**
    Open a new terminal session, navigate to the `ui` directory, and run:
    ```bash
    streamlit run ui/streamlit_app.py
    ```
    This will launch the web UI in your browser.

Now you can upload documents and ask questions through the Streamlit interface.

## üìÅ Project Structure

<img width="642" height="476" alt="Screenshot 2025-08-10 170657" src="https://github.com/user-attachments/assets/50b7caf4-f517-496e-ada5-ef8cae7aa9fa" />

## check out the video
["Demo video"](https://drive.google.com/file/d/1dZqSWwCpOTaMqN5hP0-3zmK7pEvn53kC/view?usp=sharing)
